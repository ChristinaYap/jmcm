\documentclass[article, nojss]{jss}

%\VignetteIndexEntry{Joint Mean-Covariance Modelling of Longitudinal Data}
%\VignetteEngine{R.rsp::tex}

\usepackage{amsmath}
\usepackage{graphics}

\newcommand{\CORR}{\mathsf{CORR}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{
  Jianxin Pan\\
  The University of Manchester
  \And
  Yi Pan\\
  The University of Manchester
}
\title{\pkg{jmcm}: An \proglang{R} Package for Joint Mean-Covariance Modelling of Longitudinal Data}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Jianxin Pan, Yi Pan} %% comma-separated
\Plaintitle{jmcm: An R package for Joint Mean-Covariance Modelling of Longitudinal Data} %% without formatting
\Shorttitle{\pkg{jmcm}: Joint Mean-Covariance Modelling of Longitudinal Data in R} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ Longitudinal studies commonly arise in various fields such as
  psychology, social science, economics and medical research, etc. It is of
  great importance to understand the dynamics in the mean function, covariance
  and/or correlation matrices of repeated measurements. However,
  high-dimensionality (HD) and positive-definiteness (PD) constraints are two
  major stumbling blocks in modelling of covariance and correlation matrices. It
  is evident that Cholesky-type decomposition based methods are effective in
  dealing with HD and PD problems, but those methods were not implemented in
  statistical software yet, causing a difficulty for practitioners to use.  In
  this paper, we first introduce recently developed Cholesky decomposition based
  methods for joint modelling of mean and covariance structures, namely modified
  Cholesky decomposition (MCD), alternative Cholesky decomposition (ACD) and
  hyperspherical parameterization of Cholesky factor (HPC). We then introduce
  our newly developed \proglang{R} package \pkg{jmcm} which is currently able to
  handle longitudinal data that follows a Gaussian distribution using the MCD,
  ACD and HPC methods. Demonstration is provided by running the package
  \pkg{jmcm} and comparison of those methods is made through analysing two
  real data sets.  }

\Keywords{Cholesky decomposition, covariance matrix estimator, longitudinal data, joint mean-covariance models}
%% \Plainkeywords{keywords, comma-separated, not capitalized, Java}
%% without formatting at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Jianxin Pan\\
  %Room 2.106, Alan Turing Building\\
  School of Mathematics\\
  The University of Manchester\\
  Oxford Road, Manchester, M13 9PL\\
  UK\\
  {} \\
  Telephone: +44/161/275-5864\\
  Fax: +44/161/275-5819\\
  E-mail: \email{jianxin.pan@manchester.ac.uk}\\
  URL: \url{http://www.manchester.ac.uk/research/jianxin.pan/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line
%% (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{}
%% commands.

\section[Introduction]{Introduction}
\label{sec:intro}
%% Note: If there is markup in \(sub)section, then it has to be
%% escape as above.

A longitudinal study usually involves repeated observations of the same
variables over a long period of time and is often used in psychology, sociology
and medical research. The covariance matrix plays a prominent role in analysing
data from longitudinal studies since the components of collected measurements
within the same subject are not independent. A good covariance modelling
approach improves statistical inference of the mean of interest and the
covariance structure itself may be of scientific interest in some circumstances
\citep{diggle1998nonparametric}.

However, modelling of covariance structure is challenging because the estimated
covariance matrices in general should be positive definite and there are many
parameters in covariance matrices. To overcome these two obstacles,
\cite{pourahmadi1999joint} advocated a data-driven joint mean-covariance
modelling method based on a modified Cholesky decomposition (MCD) of the
marginal within-subject covariance matrix. The decomposition leads to a
reparameterization where entries can be interpreted in terms of innovation
variances and autoregressive coefficients. See \cite{pan2003modelling} for a
related discussion. Another Cholesky-type decomposition (ACD) proposed by
\cite{chen2003random} can be understood as modelling certain innovation
variances and moving average parameters, and the method is compared with MCD in
details by \cite{pourahmadi2007cholesky}. These two Cholesky-type approaches
demonstrate parsimonious and effective strategies, but their corresponding
variance functions cannot be directly interpreted as those of the repeated
observations. Therefore additional efforts are needed for interpreting the
variance and covariance functions. More recently, \cite{zhang2015joint}
considered a regression approach based on the standard Cholesky decomposition of
the correlation matrix and the hyperspherical parameterization (HPC) of its
Cholesky factor, of which parameters are directly interpretable with respect to
variance and correlation. A brief review of these approaches is presented
in the following sections. Note this paper is not an exhaustive survey, and
many other covariance structure modelling methods are also commonly used in the
literature, see \cite{fan2016overview} for a more general overview on the
estimation of large covariance and precision matrices.

Software for analysis of longitudinal data by some conventional approaches has
been implemented in \proglang{R} \citep{rlan} environment for many years. For
instance, several packages provide functions for determining maximum likelihood
estimates of the parameters in linear mixed-effect model (LMM) that incorporates
both fixed effects and random effects in the linear predictor,
%from which the conditional mean of the response can be evaluated,
such as the \code{lme}
function in package \pkg{nlme} \citep{nlme} and the \code{lmer} function in
package \pkg{lme4} \citep{bates2015fitting}. Similar commercial statistical
programs are also available for LMM such as \code{PROC MIXED} in \proglang{SAS}
\citep{SAS-STAT}, \code{MIXED} in \proglang{SPSS} \citep{SPSS} and \code{fitlme}
in \proglang{MATLAB} \citep{MATLAB}. The method of generalized estimating
equation (GEE) \citep{liang1986longitudinal} is widely used as it focuses on
models for the mean of the correlated observations without fully specifying the
joint distribution of the responses. Several implementations of GEE are
available through package \pkg{gee} \citep{gee} and \pkg{geepack}
\citep{halekoh2006r}. Gaussian copula model provides a flexible general
framework for marginal regression analysis of continuous, discrete and
categorical responses, and is available through package \pkg{gcmr}
\citep{masarotto2012gaussian}.  However all of these procedures are based on
specific model assumptions like existence of an expectation or homogeneous
variances and are not intuitive for a joint mean-covariance modelling
framework. In this paper we focus on the joint mean-covariance modelling for
both balanced and unbalanced longitudinal data, and we present a user friendly
\proglang{R} package \pkg{jmcm} (freely available from CRAN at
\url{http://CRAN.R-project.org/package=jmcm}) that can be used to handle such
joint models. For efficiency, the core part of package \pkg{jmcm} is implemented
in compiled \proglang{C++} code using \pkg{Rcpp} \citep{eddelbuettel2011rcpp,
  eddelbuettel2013seamless} and \pkg{RcppArmadillo}
\citep{eddelbuettel2014rcpparmadillo} for numerical linear algebra. All the
implemented \proglang{R} functions are well documented with some examples. The
main objective of this paper is to introduce the joint mean-covariance modelling
approaches in package \pkg{jmcm} to a wide audiences of statisticians and
practitioners who need to analyze longitudinal data.

The rest of the paper is organized as follows. In Section 2 we present the joint
mean-covariance modelling methods, and discuss different choices for modelling
strategies of covariance structure mentioned above. Furthermore, we consider the
maximum likelihood estimations for each type of the models, with particular
emphasize on numerical optimization techniques. Section 3 provides detailed
implementation of the methods introduced in Section 2 using the package
\pkg{jmcm} and gives a brief illustration of the computational tools and Section
4 concludes the paper with further discussions.



\section[jmcm frame work]{Joint mean-covariance modelling framework}
\label{sec:jmcmframework}
\subsection{Joint mean-covariance models}
\label{subsec:bfgs}
Denote longitudinal measurements by $y_i = (y_{i1}, y_{i2}, \cdots,
y_{im_i})^\top$ ($i = 1, 2, \cdots, n$) that are collected from $n$ subjects and
measured at times points $t_i = (t_{i1}, t_{i2}, \cdots, t_{im_i})^\top$. Here
we assume the number of measurements $m_i$ and time $t_i$ are subject specific,
so that unbalanced longitudinal data sets with observations taken at irregular
time points can be modelled.

The basic linear model used in joint mean-covariance modelling frame work of
longitudinal data analysis can be described by the distribution of a
vector-valued random response variable $y_i$, which is assumed to be
multivariate normal,
$$
y_i \sim N_{m_i}(\mu_i, \Sigma_i)
$$
where $\mu_i = (\mu_{i1}, \mu_{i2}, \cdots, \mu_{im_i})^\top$ is an $m_i
\times 1$ vector and $\Sigma_i$ is an $m_i \times m_i$ within-subject covariance
matrix. The mean $\mu_i$ of $y_i$ is usually modelled by a linear regression,
\begin{equation}
  \mu_i = X_i \beta
\end{equation}
where $X_i$ denotes an $m_i \times (p+1)$ model matrix with an intercept on the
first column followed by covariates of the $i$th subject, $\beta$ is a $(p + 1)
\times 1$ regression coefficient vector. The subject-specific within-subject
covariance matrix, $\Sigma_i$, may be modelled similarly based on different
decomposition approaches, and will be discussed in detail in the following
sections.

Estimates of the joint mean-covariance model parameters $\theta$, including
$\theta_1 = \beta$ in the mean model and other unspecified parameters $\theta_2$
in the covariance matrices, can be obtained by Maximum Likelihood (ML
estimation). In particular, a maximum likelihood estimator (MLE) of the unknown
parameter vector is defined as any solution $\hat{\theta}_n$ of:
\begin{equation}
  \hat{\theta}_n = arg \min_{\theta \in \Theta} \left\{-2l(\theta)\right\}
\end{equation}
where
\begin{equation}\label{eq:n2l}
  -2l(\theta) = \sum_{i=1}^{n} \log |\Sigma_i| + \sum_{i=1}^{n} (y_i -
  \mu_i(\theta_1))^\top \Sigma_i^{-1}(\theta_2) (y_i - \mu_i(\theta_1))
\end{equation}
is the minus twice of the log-likelihood function without the constant
term. Note that it is the form of the log-likelihood function that is implemented
by default in the package, though value of the full log-likelihood including the
constant term can be easily obtained by setting a specific option before the model
fitting. After obtaining the score functions $U(\theta) = (U(\theta_1)^\top,
U(\theta_2)^\top)^\top$ based on $f(\theta) = -2l(\theta)$ by direct
calculations, we then estimate $\theta$ via the iterative quasi-Newton method
(BFGS) that solves the score equations. More specifically, we apply the
following quasi-Newton algorithm.

\begin{enumerate}
\item Select an initial value $\theta^{(0)} = ((\theta_1^{(0)})^\top,
  (\theta_2^{(0)})^\top)^\top$. Set the superscript $k = 0$ for iteration
  number. A convenient initial value for $\theta_1^{(0)} = \beta^{(0)}$ is its
  ordinary least-squares estimates, $\beta^{(0)} = (\sum_{i=1}^{n} X_i^\top
  X_i)^{-1} (\sum_{i=1}^{n} X_i^\top y_i)$ while the initial value for
  $\theta_2^0$ is set to form a diagonal covariance matrix, and will be
  discussed in details respectively with the choice of covariance structure
  models later.

\item Initialize score function $U^0 = U(\theta^0)$ and the inverse Hessian
  $H^0$ as identity matrix.

\item Update search direction (Newton step) and compute step size
  $\tilde{\lambda}$ by performing an approximate line minimization
  \begin{equation}
    p^{(k)} = -H^{(k)} U^{(k)}, \quad \tilde{\lambda} = arg \min_{0 <
      \tilde{\lambda} \leq 1} f(\theta^{(k)} + \tilde{\lambda} p^{(k)}).
  \end{equation}

\item Update $\theta$ as
  \begin{equation}
    \theta^{(k+1)} = \theta^{(k)} + \tilde{\lambda} p^{(k)}
  \end{equation}
  and then the new gradient
  \begin{equation}
    U^{(k+1)} = U(\theta^{(k+1)}).
  \end{equation}

\item Compute the difference $\theta^{(k+1)} - \theta^{(k)}$ and $U^{(k+1)} -
  U^{(k)}$ and update the inverse Hessian with the BFGS updating formula
  \begin{equation}
    \begin{aligned}
      H_{i+1} = & H_i + \frac{(\theta^{(k+1)} - \theta^{(k)}) (\theta^{(k+1)} -
        \theta^{(k)})^\top}{(\theta^{(k+1)} - \theta^{(k)})^\top (U^{(k+1)} -
        U^{(k)})} \\ & - \frac{[H^{(k)} (U^{(k+1)} - U^{(k)})] [H^{(k)}
          (U^{(k+1)} - U^{(k)})]^\top} {(U^{(k+1)} - U^{(k)})^\top H^{(k)}
        (U^{(k+1)} - U^{(k)})} \\ & + (U^{(k+1)} - U^{(k)})^\top H^{(k)}
      (U^{(k+1)} - U^{(k)}) u u^\top
    \end{aligned}
  \end{equation}
  where $u$ is defined as the following vector
  \begin{equation}
    u\equiv \frac{(\theta^{(k+1)} - \theta^{(k)})} {(\theta^{(k+1)} -
      \theta^{(k)})^\top (U^{(k+1)} - U^{(k)})} - \frac{H^{(k)} (U^{(k+1)} -
      U^{(k)})} {(U^{(k+1)} - U^{(k)})^\top H^{(k)} (U^{(k+1)} - U^{(k)})}.
  \end{equation}

\item Set $k = k + 1$ and repeat steps 3 to 5 until a pre-specified criterion is
  met.
\end{enumerate}

See \cite{press2007numerical} for a more detailed discussion of BFGS
optimization algorithm with line-search and its implementations. Note that
currently only BFGS algorithm is implemented in the package
since it proves to be one of the best quasi-Newton methods for solving smooth
unconstrained optimization problem and works very well in our problem.
Other quasi-Newton algorithms will also be implemented as alternatives in the future.
%in respond to request from users about both efficiency and convergence properties.
In practice, we find the estimates of parameters
$\theta$ of the joint mean-covariance model can be further improved by solving
the parameters one by one with other parameters fixed in the optimization, and
will be discussed in more detail in the following sections.





\subsection{Modified Cholesky decomposition (MCD)}
\label{subsec:mcddef}
The two major obstacles in modelling covariance matrices are high-dimensionality
(HD) and positive-definiteness (PD). The HD problem usually alleviated from
regressions analysis with a large number of covariates and the PD problem can be
removed by infusing regression-based ideas into Cholesky decomposition
\citep{pourahmadi2013high}. The standard Cholesky decomposition of an $m_i
\times m_i$ positive definite covariance matrix is of the following form
\begin{equation}
  \Sigma_i = C_i C_i^\top	
\end{equation}
where $C_i = (c_{ijk})$ is a lower triangular matrix with positive diagonal
elements and its entries are difficult to interpret
\citep{pinheiro1996unconstrained}. We will find that the task of statistical
interpretation can be much easier by reducing $C_i$ to unit lower triangular
matrices by post- and pre-multiply the inverse of $D_i = diag(c_{i11}, c_{i22},
\cdots, c_{i m_i m_i})$.

\vspace*{1\baselineskip}

\textit{Defining modified Cholesky decomposition (MCD)}	

The first case, post-multiplying $C_i$ by the inverse of $D_i$, leads to the
modified Cholesky decomposition (MCD) and keeps $D_i$ inside
\citep{zhang2012moving},
\begin{equation}
  \Sigma_i = (C_i D_i^{-1}) (D_i D_i) (D_i^{-1} C_i^\top) = L_i D_i^2 L_i^\top
\end{equation}
or in another more commonly used form \citep{pourahmadi1999joint},
\begin{equation}
  T_i \Sigma_i T_i^\top = D_i^2
\end{equation}
where $T_i = L_i^{-1}$ and $L_i = C_i D_i^{-1}$ can be considered as a
standardised version of $C_i$, dividing each column by its corresponding
diagonal entry \citep{maadooliat2013robust}.

The below-diagonal entries of $T_i$ are the negatives of the so-called
generalized autoregressive parameters (GARPs), $\phi_{ijk}$, in
\begin{equation} \label{eq:mcdinter1}
  y_{ij} = \mu_{ij} + \sum_{k=1}^{j-1} \phi_{ijk} (y_{ik} - \mu_{ik}) + \epsilon_{ij}
\end{equation}
the AR model for the actual measurements on subject $i$. The diagonal entries of
$D_i^2$ are the innovation variance $\sigma_{ij}^2 = Var(\epsilon_{ij})$, see
\cite{pourahmadi1999joint} for the details. It is helpful to invert
Equation~\ref{eq:mcdinter1} by using $y_{i1} = \epsilon_{i1}$ and repeating
substitution for $y_{it}$ in terms of $\epsilon_{it}$ to obtain
\begin{equation}
  y_{ij} - \mu_{ij} = \epsilon_{ij} + \sum_{k=1}^{j-1} \tilde{\phi}_{ijk}
  \epsilon_{ik} \quad (j = 2, \cdots, m_i)
\end{equation}
where the matrix form reveals $L_i=(\tilde{\phi}_{ijk})$ so that its entries on
the $j$th row can be interpreted as regression parameters when $y_{ij}$ is
regressed on the present and past innovations
$\epsilon_{ij},\epsilon_{i(j-1)},\cdots,\epsilon_{i1}$. Then we can prove
\begin{equation}
  \COV(y_{is}, y_{it}) = \sum_{k=1}^{\min(s,t)} \tilde{\phi}_{isk}
  \tilde{\phi}_{itk} \sigma_{ik}^2
\end{equation}
by setting $\tilde{\phi}_{ijj} = 1$ and $\tilde{\phi}_{ijk} = 0$ for $j < k$ and
$1 \leq s,t \leq m_i$. Thus, the correlation coefficient between $y_{is}$ and
$y_{it}$ depends on both the $\tilde{\phi}_{ijk}$'s and the $\sigma_{ij}^2$'s.

\vspace*{1\baselineskip}

\textit{Maximum likelihood estimation of MCD}

Using the idea of linear models and employing covariates as in
\cite{pan2003modelling}, the unconstrained parameters $\zeta_{ij} \equiv \log
\sigma_{ij}^2$ and $\phi_{ijk}$ are modelled as
\begin{equation} \label{eq:mcdmod}
  \zeta_{ij} = z_{ij}^\top \lambda, \quad \phi_{ijk} = w_{ijk}^\top \gamma
\end{equation}
where $z_{ij}$ and $w_{ijk}$ are $(d+1) \times 1$ and $(q+1) \times 1$ vectors
of covariates, $\lambda = (\lambda_0, \lambda_1, \cdots, \lambda_d)^\top$ and
$\gamma = (\gamma_0, \gamma_1, \cdots, \gamma_q)^\top$ are unknown parameters
for the innovation variances and autoregressive coefficients, respectively.

Under model in (\ref{eq:mcdmod}), the minus twice log-likelihood function,
except for a constant, is given by
\begin{equation} \label{eq:n2l1}
  -2l = \sum_{i=1}^{n} \log |T_i^{-1} D_i^2 T_i^{-\top}| + \sum_{i=1}^{n}
  r_i^\top T_i^\top D_i^{-2} T_i r_i
\end{equation}
where $r_{ij} = y_{ij} - x_{ij}^\top \beta$ is the $j$th element of $r_{i} =
y_{i} - X_{i} \beta$, the vector of residuals for $i$th subject.

The maximum likelihood estimating equations for $\beta$, $\lambda$ and $\gamma$
become
\begin{equation} \label{eq:mcdgrad}
  \left\{
    \begin{aligned}
      &U_1(\beta) = \sum_{i=1}^{n} X_i^\top \Sigma_i^{-1} (y_i - X_i \beta)\\
      &U_2(\lambda) = \frac{1}{2} \sum_{i=1}^{n} Z_i^\top (D_i^{-2} e_i - 1_{m_i}) \\
      &U_3(\gamma) = \sum_{i=1}^{n} G_i^\top D_i^{-2} (r_i - G_i \gamma)
    \end{aligned}
  \right.	
\end{equation}
where the matrix $G_i$, of order $m_i \times (q+1)$, has typical row
$g_{ij}^\top = \sum_{k=1}^{j-1} r_{ik} w_{ijk}^\top$. Also, $Z_i = (z_{i1}^\top,
z_{i2}^\top, \cdots, z_{im_i}^\top)^\top$, $e_i = (e_{i1}, e_{i2}, \cdots,
e_{im_i})^\top$ with $e_{ij} = (r_{ij} - \hat{r}_{ij})^2$ and $\hat{r}_{ij} =
\sum_{k=1}^{j-1} \phi_{ijk} r_{ik}$, are the $m_i \times (d + 1)$ matrix of
covariates and the $m_i \times 1$ vector of squared fitted residuals
respectively, and $1_{m_i}$ is the $m_i \times 1$ vector of 1's.

The initial guess $\beta^{(0)}$ can be set by employing a simple linear
regression:

\begin{CodeChunk}
\begin{CodeInput}
R> lm.obj <- lm(Y ~ X - 1)
R> bta0 <- coef(lm.obj)
\end{CodeInput}
\end{CodeChunk}	

After we exact residuals from the linear model, the starting value
$\lambda^{(0)}$ is obtained by fitting its linear regression model in
(\ref{eq:mcdmod}) while $\gamma^{(0)}$ is simply assumed to be a vector of 0's
so that $T_i$ is constructed as an identity matrix :

\begin{CodeChunk}
\begin{CodeInput}
R> resid(lm.obj) -> res
R> lmd0 <- coef(lm(log(res ^ 2) ~ Z - 1))
R> gma0 <- rep(0, lgma)
\end{CodeInput}
\end{CodeChunk}

We then estimate $\theta$ by minimizing expression in (\ref{eq:n2l1}) via the
iterative quasi-Newton algorithm, as explained in Section~\ref{subsec:bfgs},
after substitution of $U(\theta)$ by $(-2U_1(\beta)^\top, -2U_2(\lambda)^\top,
-2U_3(\gamma)^\top)^\top$.

Since the solutions satisfy Equation~\ref{eq:mcdgrad} and the parameters
$\beta$, $\lambda$, $\gamma$ are asymptotically independent
\citep{ye2006modelling}, the three parameters can also be sequentially solved
one by one with other parameters kept fixed. More specifically, we apply the
following algorithm.

\begin{enumerate}
\item Initialize the parameters as $\theta^{(0)} =
  ((\beta^{(0)})^\top,(\lambda^{(0)})^\top, (\gamma^{(0)})^\top)^\top$. Set
  $k=0$.

\item Compute $\Sigma_i$ by using $\lambda^{(k)}$ and $\gamma^{(k)}$. Update
  $\beta$ as
  $$ \beta = (\sum_{i=1}^{n} X_i^\top \Sigma_i^{-1} X_i)^{-1} \sum_{i=1}^{n}
  X_i^\top \Sigma_i^{-1} y_i.
  $$

\item Given $\beta = \beta^{(k+1)}$ and $\gamma = \gamma^{(k)}$, update
  $\lambda$ via the iterative quasi-Newton algorithm after substitution of
  $f(\theta) = -2l(\theta)$ by $f(\lambda)$ and $U(\theta)$ by $-2U_2(\lambda)$
  since there is no explicit form for the solution of $\lambda$.

\item Given $\beta = \beta^{(k+1)}$ and $\lambda= \lambda^{(k+1)}$,
  update $\gamma$ as
  $$ \gamma = (\sum_{i=1}^{n} G_i^\top D_i^{-2} G_i)^{-1} \sum_{i=1}^{n}
  G_i^\top D_i^{-2} r_i.
  $$		

\item Update search direction as
  $$
  p^{(k)} = \theta^{(k+1)} - \theta^{(k)},
  $$ Compute step size $\tilde{\lambda}$ by performing an approximate line
  minimization
  $$ \tilde{\lambda} = arg \min_{0 < \tilde{\lambda} \leq 1} f(\theta^{(k)} +
  \tilde{\lambda} p^{(k)}).
  $$

\item Update $\theta^{(k+1)}$ again as
  $$
  \theta^{(k+1)} = \theta^{(k)} + \tilde{\lambda} p^{(k)}.
  $$

\item Set $k = k + 1$ and repeat steps 2 to 6 until a pre-specified criterion is
  met.

\end{enumerate}

\subsection{Alternative Cholesky decomposition (ACD)}
\label{subsec:acddef}

\vspace*{1\baselineskip}

\textit{Defining alternative Cholesky decomposition (ACD)}	

The second case, pre-multiplying $C_i$ by the inverse of $D_i$, leads to
alternative Cholesky decomposition (ACD) \citep{chen2003random} and keeps $D_i$
outside,
$$ \Sigma_i = D_i (D_i^{-1} C_i) (C_i^\top D_i^{-1}) D_i = D_i \tilde{L}_i
\tilde{L}_i^\top D_i
$$ where $\tilde{L}_i = D_i^{-1} C_i$ is obtained from a slightly different
standardised $C_i$, dividing each row by its corresponding diagonal entry
\citep{maadooliat2013robust}.

For statistical interpretation of the below-diagonal entries of $\tilde{L}_i$,
it is clear that $D_i^{-1} (y_i - \mu_i)$ has $\tilde{L}_i \tilde{L}_i^\top$ as
the standard Cholesky decomposition of its covariance matrix and $\epsilon_i =
(D_i \tilde{L}_i)^{-1} (y_i - \mu_i)$, its vector of innovations, has
$\COV(\epsilon_i) = I_{m_i}$. Thus, with $\tilde{L}_i = (\tilde{\phi}_{ijk})$,
$D_i = (\sigma_{ij})$ and from $D_i^{-1} (y_i - \mu_i) = \tilde{L}_i \epsilon_i$
we obtain variable-order, MA representation for the standardized $(y_{ij} -
\mu_{ij}) / \sigma_{ij}$ as
\begin{equation}
  (y_{ij} - \mu_{ij}) / \sigma_{ij} = \epsilon_{ij} + \sum_{k=1}^{j-1}
  \tilde{\phi}_{ijk} \epsilon_{ik}
\end{equation}
Then we can prove
\begin{equation}
  \COV(y_{is}, y_{it}) = \sigma_{is} \sigma_{it} \sum_{k=1}^{\min(s,t)}
  \tilde{\phi}_{itk} \tilde{\phi}_{isk}
\end{equation}
for any $1 \leq s,t \leq m_i$, so that the correlation coefficient between
$y_{is}$ and $y_{it}$ given by
\begin{equation}
  \CORR(y_{is}, y_{it}) = \frac{\sum_{k=1}^{\min(s,t)} \tilde{\phi}_{itk}
    \tilde{\phi}_{isk}}{\sqrt{(\sum_{k=1}^s \tilde{\phi}_{isk}^2 \sum_{k=1}^t
      \tilde{\phi}_{itk}^2})}
\end{equation}
is determined solely by $\tilde{\phi}_{ijk}$'s.

\vspace*{1\baselineskip}

\textit{Maximum likelihood estimation of ACD}

Following the similar approach in \cite{pan2003modelling}, the log-innovation
variance $\zeta_{ij} = \log \sigma_{ij}^2$ and moving average parameters
$\tilde{\phi}_{ijk}$ in ACD are modelled as
\begin{equation} \label{eq:acdmod}
  \zeta_{ij} = z_{ij}^\top \lambda, \quad \tilde{\phi}_{ijk} = v_{ijk}^\top \gamma
\end{equation}
where $z_{ij}$ and $v_{ijk}$ are $(d+1) \times 1$ and $(q+1) \times 1$ vectors
of covariates, $\lambda = (\lambda_0, \lambda_1, \cdots, \lambda_d)^\top$ and
$\gamma = (\gamma_0, \gamma_1, \cdots, \gamma_q)^\top$ are unknown parameters
for the innovation variances and moving average regression coefficients,
respectively.

Under model in (\ref{eq:acdmod}), the minus twice log-likelihood function,
except for a constant, is given by
\begin{equation} \label{eq:n2l2}
  -2l = \sum_{i=1}^{n} \log |D_i \tilde{L}_i \tilde{L}_i^\top D_i| +
  \sum_{i=1}^{n} r_i^\top D_i^{-1} \tilde{L}_i^{-\top} \tilde{L}_i^{-1} D_i^{-1}
  r_i
\end{equation}
where $r_{ij} = y_{ij} - x_{ij}^\top \beta$ is the $j$th element of $r_{i} =
y_{i} - X_{i} \beta$, the vector of residuals for $i$th subject.

The score functions can be obtained and simplified as
\begin{equation} \label{eq:acdgrad}
  \left\{
    \begin{aligned}
      &U_1(\beta) = \sum_{i=1}^{n} X_i^\top \Sigma_i^{-1} (y_i - X_i
      \beta)\\ &U_2(\lambda) = \frac{1}{2} \sum_{i=1}^{n} Z_i^\top (h_i -
      1_{m_i}) \\ &U_3(\gamma) = \sum_{i=1}^{n} (\epsilon_i^\top \otimes
      I_{m_i}) \frac{\partial \tilde{L}_i^\top}{\partial \gamma}
      \tilde{L}_i^{-\top} \epsilon_i
    \end{aligned}
  \right.	
\end{equation}
where $Z_i = (z_{i1}^\top, z_{i2}^\top, \cdots, z_{im_i}^\top)^\top$, $h_i =
diag(\tilde{L}_i^{-1} D_i^{-1} r_i r_i^\top D_i^{-1})$, $\epsilon_i =
(\epsilon_{i1}, \cdots, \epsilon_{im_i})^\top = \tilde{L}_i^{-1} D_i^{-1} r_i$,
thus $\epsilon_{i1}, \cdots, \epsilon_{im_i}$ are independent standard normal
random variables, and $I_{m_i}$ is an $m_i \times m_i$ identity matrix.

Since covariance structure of MCD and ACD are quite close, the initial guess of
parameters $\beta^{(0)}$, $\lambda^{(0)}$ and $\gamma^{(0)}$ in ACD can be
obtained using the same approach described in initial parameter setting of MCD:

\begin{CodeChunk}
\begin{CodeInput}
R> lm.obj <- lm(Y ~ X - 1)
R> bta0 <- coef(lm.obj)
R> resid(lm.obj) -> res
R> lmd0 <- coef(lm(log(res ^ 2) ~ Z - 1))
R> gma0 <- rep(0, lgma)
\end{CodeInput}
\end{CodeChunk}

We then estimate $\theta$ by minimizing expression in (\ref{eq:n2l2}) via the
iterative quasi-Newton algorithm, as explained in Section~\ref{subsec:bfgs},
after substitution of $U(\theta)$ by $(-2U_1(\beta)^\top, -2U_2(\lambda)^\top,
-2U_3(\gamma)^\top)^\top$.

Since the solutions satisfy Equation~\ref{eq:acdgrad} and the parameters
$\lambda$ and $\gamma$ are not asymptotically orthogonal
\citep{maadooliat2013robust}, the three parameters can be split into two groups,
$\theta_1 = \beta$ and $\theta_2 = (\lambda^\top, \gamma^\top)^\top$ and can be
sequentially solved one by one with other parameter kept fixed. More
specifically, we apply the following algorithm.

\begin{enumerate}
\item Initialize the parameters as $\theta^{(0)} = ((\theta_1^{(0)})^\top,
  (\theta_2^{(0)})^\top)^\top = ((\beta^{(0)})^\top,(\lambda^{(0)})^\top,
  (\gamma^{(0)})^\top)^\top$. Set $k=0$.

\item Compute $\Sigma_i$ by using $\lambda^{(k)}$ and $\gamma^{(k)}$. Update
  $\theta_1 = \beta$ as
  $$ \beta = (\sum_{i=1}^{n} X_i^\top \Sigma_i^{-1} X_i)^{-1} \sum_{i=1}^{n}
  X_i^\top \Sigma_i^{-1} y_i.
  $$

\item Given $\beta = \beta^{(k+1)}$, update $\theta_2$ via the iterative
  quasi-Newton algorithm after substitution of $f(\theta) = -2l(\theta)$ by
  $f(\theta_2)$ and $U(\theta)$ by $(-2U_2(\lambda)^\top,
  -2U_3(\gamma)^\top)^\top$.

\item Update search direction as
  $$
  p^{(k)} = \theta^{(k+1)} - \theta^{(k)},
  $$ Compute step size $\tilde{\lambda}$ by performing an approximate line
  minimization
  $$ \tilde{\lambda} = arg \min_{0 < \tilde{\lambda} \leq 1} f(\theta^{(k)} +
  \tilde{\lambda} p^{(k)}).
  $$

\item Update $\theta^{(k+1)}$ again as
  $$
  \theta^{(k+1)} = \theta^{(k)} + \tilde{\lambda} p^{(k)}.
  $$

\item Set $k = k + 1$ and repeat steps 2 to 5 until a pre-specified criterion is
  met.

\end{enumerate}	

\subsection{Hyperspherical parameterization of Cholesky factor (HPC)}
\label{subsec:hpcdef}	

Even though modified Cholesky decomposition (MCD) \citep{pourahmadi1999joint}
and alternative Cholesky decomposition (ACD) \citep{chen2003random} provide
parsimonious unconstrained and statistically interpretable parameterization of a
covariance matrix, the innovation variance is not same as the marginal variances
of the repeated measurements within the same subject.

\vspace*{1\baselineskip}

\textit{Defining hyperspherical parameterization of Cholesky factor(HPC)}	

It is well known that variance-correlation decomposition has the form below
\begin{equation}
  \Sigma_i = H_i R_i H_i
\end{equation}
where $H_i = diag(\sigma_{i1}, \sigma_{i2}, \cdots, \sigma_{im_i})$ with
$\sigma_{ij}$ being the standard deviation of $j$th measurement for subject $i$
and $R_i = (\rho_{ijk})_{j,k=1}^{m_i}$ is the correlation matrix of $y_i$ with
$\rho_{ijk} = \CORR(y_{ij}, y_{ik})$ being the correlation between the $j$th and
$k$th observations of the $i$th subject. By using this decomposition, one can
directly model the variances and correlations of observations separately.

Not surprisingly, the development of a regression method to model the
correlation structure proves to be difficult. Specifically, a correlation matrix
must be positive semidefinite and symmetric with 1's as the main diagonal
entries and values between -1 and 1 as the off-diagonal entries. The new
challenge is mitigated by employ the standard Cholesky decomposition on
correlation matrix $R_i$,
\begin{equation}
  R_i = B_i B_i^\top
\end{equation}
and parameterize its Cholesky factor $B_i$ via hyperspherical co-ordinates (HPC)
\citep{zhang2015joint},
\[
B_i =
\begin{bmatrix}
  1 & 0 & 0 & \dots & 0 \\
  c_{i21} & s_{i21} & 0 & \dots  & 0 \\
  c_{i31} & c_{i32} s_{i31} & s_{i32} s_{i31} & \dots  & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  c_{im_i1} & c_{im_i2} s_{im_i1} & c_{im_i3} s_{im_i2} s_{im_i1} & \dots  & \prod_{l=1}^{m_i - 1} s_{im_il}\\
\end{bmatrix}
\]
where $c_{ijk} = \cos(\theta_{ijk})$ and $s_{ijk} = \sin(\theta_{ijk})$.

Equivalently, the non-zeros entries in the lower triangular matrix $B_i =
(b_{ijk})$ are given as $b_{i11} = 1$, $b_{ij1} = c_{ij1} = \cos(\theta_{ij1})$
for $j = 1, 2, \cdots, m_i$ and
$$ b_{ijk}=\left\{
  \begin{aligned}
    & \cos(\theta_{ijk}) \prod_{l=1}^{k-1} \sin(\theta_{ijl}), & & {2 \leq k < j \leq m_i},\\
    & \prod_{l=1}^{k-1} \sin(\theta_{ijl}), & & k = j, j = 2, \cdots, m_i.
  \end{aligned}
\right.
$$
where $\theta_{ijk}$ are some angles in $[0, \pi)$ \citep{rapisarda2007parameterizing}

\vspace*{1\baselineskip}

\textit{Maximum likelihood estimation of HPC}

The correlation matrix $R_i$ is guaranteed positive semi-definite since it is
constructed by its corresponding standard Cholesky factor $B_i$ and the angle
parameters in $B_i$ are unconstrained except that $\theta_{ijk} \in [0,
  \pi)$. We are free to model the log-variances and angles through regression by
  using some covariates
\begin{equation} \label{eq:hpcmod}
  \log{\sigma_{ij}^2} = z_{ij}^\top \lambda, \quad \theta_{ijk} = g_{ijk}^\top \gamma
\end{equation}

As for the range of $\theta_{ijk}$, our experience from data analysis and
simulation study indicates all the estimated $\theta_{ijk}$s fall in the range
$[0, \pi)$. Transformation such as the inverse tangent transformation can be
  applied to ensure that $\theta_{ijk}$ definitely falls in $[0, \pi)$, and can be
    implemented in a future version. Under model in (\ref{eq:hpcmod}), the minus
    twice log-likelihood function, except for a constant, is given by
\begin{equation} \label{eq:n2l3}
  -2l = \sum_{i=1}^{n} \log |H_i B_i B_i^\top H_i| + \sum_{i=1}^{n} r_i'
  H_i^{-1} B_i^{-\top} B_i^{-1} H_i^{-1} r_i
\end{equation}
where $r_{ij} = y_{ij} - x_{ij}^\top \beta$ is the $j$th element of $r_{i} =
y_{i} - X_{i} \beta$, the vector of residuals for $i$th subject.

The score functions can be obtained and simplified as
\begin{equation} \label{eq:hpcgrad}
  \left\{
    \begin{aligned}
      & U_1(\beta) = \sum_{i=1}^{n} X_i^\top \Sigma_i^{-1} (y_i - X_i \beta) \\
      & U_2(\lambda) = \frac{1}{2} \sum_{i=1}^{n} Z_i^\top(h_i - 1_{m_i}) \\
      & U_3(\gamma) = \sum_{i=1}^{n} ((\epsilon_i^\top \otimes I_{m_i}) \frac{\partial B_i^\top}{\partial \gamma} B_i^{\top -1} \epsilon_i - \sum_{j=1}^{m_i} \frac{\partial \log B_{ijj}}{\partial \gamma})
    \end{aligned}
  \right.
\end{equation}
where $Z_i = (z_{i1}^\top, z_{i2}^\top, \cdots, z_{im_i}^\top)^\top$, $h_i =
diag(B_i^{-1} H_i^{-1} r_i r_i^\top H_i^{-1})$, $\epsilon_i = (\epsilon_{i1},
\cdots, \epsilon_{im_i})^\top = B_i^{-1} H_i^{-1} r_i$, thus $\epsilon_{i1},
\cdots, \epsilon_{im_i}$ are independent standard normal random variables, and
$I_{m_i}$ is an $m_i \times m_i$ identity matrix.

The initial guess $\beta^{(0)}$ can be set by employing a simple linear regression:

\begin{CodeChunk}
\begin{CodeInput}
R> lm.obj <- lm(Y ~ X - 1)
R> bta0 <- coef(lm.obj)
\end{CodeInput}	
\end{CodeChunk}

After we exact residuals from the linear model, the starting value
$\lambda^{(0)}$ is obtained by fitting its linear regression model in
(\ref{eq:hpcmod}) while $\gamma^{(0)}$ is simply assumed to be a vector whose
first element is $\frac{1}{2}\pi$ and followed by 0's so that $B_i$ is
constructed as an identity matrix :

\begin{CodeChunk}
\begin{CodeInput}
R> resid(lm.obj) -> res
R> lmd0 <- coef(lm(log(res ^ 2) ~ Z - 1))
R> gma0 <- c(pi / 2, rep(0, lgma-1))
\end{CodeInput}	
\end{CodeChunk}

We then estimate $\theta$ by minimizing expression in (\ref{eq:n2l3}) via the
iterative quasi-Newton algorithm, as explained in Section~\ref{subsec:bfgs},
after substitution of $U(\theta)$ by $(-2U_1(\beta)^\top, -2U_2(\lambda)^\top,
-2U_3(\gamma)^\top)^\top$.

Since the solutions satisfy Equation~\ref{eq:hpcgrad} and the parameters
$\lambda$ and $\gamma$ are not asymptotically independent
\citep{zhang2015joint}, the three parameters can be split into two groups,
$\theta_1 = \beta$ and $\theta_2 = (\lambda^\top, \gamma^\top)^\top$ and can be
sequentially solved one by one with other parameters kept fixed. More
specifically, we apply the following algorithm.

\begin{enumerate}
\item Initialize the parameters as $\theta^{(0)} = ((\theta_1^{(0)})^\top,
  (\theta_2^{(0)})^\top)^\top = ((\beta^{(0)})^\top,(\lambda^{(0)})^\top,
  (\gamma^{(0)})^\top)^\top$. Set $k = 0$.

\item Compute $\Sigma_i$ by using $\lambda^{(k)}$ and $\gamma^{(k)}$. Update
  $\theta_1 = \beta$ as
  $$ \beta = (\sum_{i=1}^{n} X_i^\top \Sigma_i^{-1} X_i)^{-1} \sum_{i=1}^{n}
  X_i^\top \Sigma_i^{-1} y_i.
  $$

\item Given $\beta = \beta^{(k+1)}$, update $\theta_2$ via the iterative
  quasi-Newton algorithm after substitution of $f(\theta) = -2l(\theta)$ by
  $f(\theta_2)$ and $U(\theta)$ by $(-2U_2(\lambda)^\top,
  -2U_3(\gamma)^\top)^\top$.

\item Update search direction as
  $$
  p^{(k)} = \theta^{(k+1)} - \theta^{(k)},
  $$ Compute step size $\tilde{\lambda}$ by performing an approximate line
  minimization
  $$ \tilde{\lambda} = arg \min_{0 < \tilde{\lambda} \leq 1} f(\theta^{(k)} +
  \tilde{\lambda} p^{(k)}).
  $$

\item Update $\theta^{(k+1)}$ again as
  $$
  \theta^{(k+1)} = \theta^{(k)} + \tilde{\lambda} p^{(k)}.
  $$

\item Set $k = k + 1$ and repeat steps 2 to 5 until a pre-specified criterion is
  met.

\end{enumerate}	


\subsection{Comparison of MCD, ACD and HPC}	
\label{subsec:comparison}

For modelling the covariance and correlation structure, the three discussed
Cholesky-type decomposition-based approaches have been demonstrated to be
effective in the sense that estimated covariance and correlation are guaranteed
positive (semi)definite, and number of parameters is largely reduced through
regression techniques.

It is clear that MCD and ACD has a more close relationship since they are
constructed similarly through standardize the Cholesky factor $C_i$, and the
resulting unconstrained parameters have a nice statistical interpretation in
terms of innovation variance, autoregressive and moving average parameters
respectively. The main drawbacks of these two approaches are the potential needs
for a natural order (e.g., time series), which makes it difficult to find a
reasonable statistical interpretation and may result in different estimation of
covariance and correlation matrix with each single ordering. A recent
application of Cholesky-based approach for estimating covariance matrix of
multiple stocks within a portfolio and more detailed discussion of ordering
problem can be find in \cite{dellaportas2012cholesky},
\cite{pedeli2015two}. Additional effort and extra care are needed in practice
for interpreting their corresponding variance and correlation
functions. Moreover, owning to the decomposition, resulting correlation function
of MCD depends on both the innovation variance and autoregressive parameters,
indicating MCD is not robust against the misspecification of innovation variance
when correlation is the main interest \citep{maadooliat2013robust}. We also need
to note that MCD is most computationally efficient among three approaches due to
the fact that its Fisher information matrix is block diagonal
\citep{ye2006modelling}.

The parameterization of HPC is very attractive because the resulting parameters
are unconstrained and directly interpretable with respect to the variances and
correlations. The angles in the Cholesky factor of correlation matrix have a
geometric connection with correlations. However, modelling covariance and
correlation using HPC can be computationally expensive since it transforms the
problem of estimating Cholesky factor into the one that actually first estimates
a matrix consists of angles. See details in \cite{zhang2015joint}.




\section[examples]{Examples of use}
\label{sec:examples}

\subsection{Analysis of a balanced longitudinal dataset}
\label{subsec:cattlebal}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
In this section, we provide our first example that illustrates how to apply
joint mean-covariance models in analysing a balanced longitudinal data by using
\pkg{jmcm}. \cite{kenward1987method} reported an experiment on cattle intestinal
paasites controls in which the cattle were assigned randomly to two treatment
groups A and B, and their weights were recorded. Thirty animals received
treatment A and another thirty received treatment B. The animals were weighted
$n = 11$ times over a 133-day period; the first 10 measurements on each animal
were made at two-week intervals and the final measurement was made one week
later. Since no observation was missing, it is considered to be a balanced
longitudinal dataset. The data is loaded simply using the \code{data()}
instruction:

\begin{CodeChunk}
\begin{CodeInput}	
R> library("jmcm")
R> data("cattle", package = "jmcm")
R> head(cattle)
\end{CodeInput}
\begin{CodeOutput}			
  id day group weight
1  1   0     A    233
2  1  14     A    224
3  1  28     A    245
4  1  42     A    258
5  1  56     A    271
6  1  70     A    287
\end{CodeOutput}
\end{CodeChunk}

We present in Figure~\ref{fig:cattledata} the subject-specific longitudinal
profiles of the cattle data using following code:
\begin{CodeChunk}
\begin{CodeInput}
R> library("lattice")
R> xyplot(weight ~ day | group, group = id, data = cattle, xlab = "days",
+    ylab = "weight", col = 1, type = "l")
\end{CodeInput}
\end{CodeChunk}
and observes that in both groups the variability of weights seems to increase
over time with a severe weight loss on the final measurement in group B.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.80\textwidth]{cattledata}
  \caption{\label{fig:cattledata}Subject-specific weight against time for group
    A and B.}
\end{figure}

Following \cite{pourahmadi1999joint}, \cite{pan2003modelling},
\cite{pan2006regression}, \cite{pan2007modelling} and \cite{zhang2015joint}, we re-analysed group A data
by using a saturated mean model with the common measurement time rescaled to $t
= 1, 2, \cdots, 10, 10.5$. The Bayesian Information Criterion (BIC), which is
closely related to Akaike Information Criterion (AIC) and introduces a larger
penalty term for the number of parameters in the model than the AIC aiming to
solve the problem of over-fitting, is used as the criterion to select the optimum model
\begin{equation}
  BIC(p, d, q) = -2 \hat{l}_{max} / n + (p + d + q + 3) \log(n) / n
\end{equation}
where $p$, $d$ and $q$ are respectively the orders of three polynomials and
$\hat{l}_{max}$ is the value of maximum log-likelihood function for the given
order. By default, the value of likelihood does not include the constant term as
defined in Equation~(\ref{eq:n2l}) but it can be switched to the full likelihood
containing the constant term easily by explicitly specifying
\code{control = jmcmControl(ignore.const.term = F)}
in \code{jmcm} function.


The basic use of \code{jmcm} is to indicate the model formula, data, choice of
poly(p, d, q) and covariance structure model. For example, a joint
mean-covariance model based on modified Cholesky decomposition (MCD) are
estimated using:
\begin{CodeChunk}
\begin{CodeInput}
R> cattleA <- cattle[cattle$group=='A', ]
R> fit1 <- jmcm(weight | id | I(day/14 + 1) ~ 1 | 1, data = cattleA,
+    triple = c(8, 3, 4), cov.method = 'mcd')
R> fit1
\end{CodeInput}
\begin{CodeOutput}	
Joint mean-covariance model based on MCD ['jmcmMod']
Formula: weight | id | I(day/14 + 1) ~ 1 | 1
Poly: c(8, 3, 4)
Data: cattleA
	
logLik: -771.0007
BIC: 53.4408
	
Mean Parameters:
[1]   1.832e+02   1.244e+02  -1.403e+02   7.881e+01  -2.362e+01   4.071e+00
[7]  -4.052e-01   2.162e-02  -4.787e-04
Innovation Variance Parameters:
[1]   5.366409  -0.878890   0.132427  -0.006371
Autoregressive Parameters:
[1]   0.297055   0.619888  -0.396189   0.069150  -0.003696	
\end{CodeOutput}
\end{CodeChunk}

The \proglang{R} package \pkg{Formula} of \cite{zeileis2010extended} is
used to extract information from a two-sided linear formula object which
is used to describe both longitudinal data and covariates of the model,
with the response, subject id and observation time point on the left of a
"$\sim$" operator separated by vertical bars ("|") and covariates for the
mean model and innovation variance, also separated by a "|" operator, on
the right. Here both covariates for mean model and innovation variance are
marked as 1, and only time is used to construct design matrices. Optimal
model selection involves identifying the best integer triple poly(p, d,
q), specified by option \code{triple}, representing the degrees of three
polynomial functions for the mean structure, log innovation variance and
autoregressive coefficients respectively. To make the model fitting
comparable with the results reported in the literature, in this paper we
focus on the fitting using polynomials in time. The use of other
covariates is also possible and will be demonstrated later. By default,
the \code{jmcm} function uses the profile likelihood for having a better
estimating result. Alternatively, non-profile method can be applied by specifying
\code{control = jmcmConstrol(profile = F)}. When the estimation for the
fitted model is ready, an object of the \code{S4} class \code{jmcmMod}
is returned from the function and it automatically displays the
basic information by calling the generic \code{print} function. The
\code{getJMCM} function can be used to extract various objects
(e.g., estimation of mean vector and covariance matrix) from a fitted joint
mean-covariance model. In this example, the global optimum triple
poly(8,3,4) reported in \cite{pan2003modelling} is modelled, produced a
better result with $\hat{l}_{max}$ = -771.0007 and BIC = 53.4408.

Since it is a balanced longitudinal dataset, we produced the sample
regressograms and fitted curves for the cattle data using the following
function:
\begin{CodeChunk}
  \begin{CodeInput}
R> regressogram(fit1, time = 1:11)
  \end{CodeInput}
\end{CodeChunk}

By examining the log innovation variance versus time in Figure~\ref{fig:mcdreg},
it is clear that curvature pattern is well captured by the fitted polynomial
function curve. Figure~\ref{fig:mcdreg} also indicates a good fit for
autoregressive coefficients by examining the autoregressive coefficient versus
time lag between measurements and the fitted curve.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.80\textwidth]{mcdreg}
  \caption{\label{fig:mcdreg}Group A analysis of Kenward's cattle data. Sample
    regressograms and MCD model fits based on the triple ploy(8, 3, 4) for log
    innovation variances (left) and autoregressive coefficients (right).}
\end{figure}

Same triple poly(8,3,4), representing the degrees of three polynomial functions
for the mean structure, the log innovation variance and moving average
coefficients respectively, is used in joint mean-covariance model fitting based
on ACD for cattle data. The covariance structure option should be specified as
\code{cov.method = 'acd'}. By comparing the maximized log-likelihood and BIC for
MCD modelling, we clearly see that ACD method produces a larger likelihood
$\hat{l}_{max} = -747.6994$ and a smaller BIC-value 51.8873.

\begin{CodeChunk}
  \begin{CodeInput}
R> fit2 <- jmcm(weight | id | I(day/14 + 1) ~ 1 | 1, data = cattleA,
+    triple = c(8, 3, 4), cov.method = 'acd')
R> fit2
  \end{CodeInput}
  \begin{CodeOutput}
Joint mean-covariance model based on ACD ['jmcmMod']
Formula: weight | id | I(day/14 + 1) ~ 1 | 1
Poly: c(8, 3, 4)
Data: cattleA
	
logLik: -747.6994
BIC: 51.8873
	
Mean Parameters:
[1]   1.784e+02   1.360e+02  -1.511e+02   8.400e+01  -2.503e+01   4.299e+00
[7]  -4.267e-01   2.272e-02  -5.020e-04
Innovation Variance Parameters:
[1]   4.959622  -0.625990   0.084538  -0.003963
Moving Average Parameters:
[1]   0.5216849   0.4795778  -0.1049448   0.0105871  -0.0003623
  \end{CodeOutput}
\end{CodeChunk}

Regressograms for ACD can be produced by the same command:
\begin{CodeChunk}
  \begin{CodeInput}
R> regressogram(fit2, time = 1:11)
  \end{CodeInput}
\end{CodeChunk}

By examining the log innovation variance versus time (left) and moving average
coefficient versus time lag (right) in Figure~\ref{fig:acdreg}, similar
conclusion can be drawn that the proposed polynomial model fitting well captured
trend in sample regressogram.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.80\textwidth]{acdreg}
  \caption{\label{fig:acdreg}Group A analysis of Kenward's cattle data. Sample
    regressograms and ACD model fits based on the triple ploy(8, 3, 4) for log
    innovation variances (left) and moving average coefficients (right).}
\end{figure}

When the joint mean-covariance model approach based on HPC is applied to the
cattle data, the covariance structure option should be specified as
\code{cov.method = 'hpc'}. Same two-sided linear formula object is used to
describe both longitudinal data and covariates of the model, but on the right of
a "$\sim$" operator, covariates for mean model and variance is specified on the
right side of the operator instead of mean model and innovation variance in MCD
and ACD. The integer triple poly(p, d, q) is specified by option \code{triple},
representing the degrees of three polynomial functions for the mean structure,
log variance and angles respectively. More specifically, the optimal triple
ploy(8, 2, 2) of HPC model fitting reported in \cite{zhang2015joint} can be
reproduced using the following command:

\begin{CodeChunk}
  \begin{CodeInput}
R> fit3 <- jmcm(weight | id | I(day/14 + 1) ~ 1 | 1, data = cattleA,
+    triple = c(8, 2, 2), cov.method = 'hpc')
R> fit3
  \end{CodeInput}
  \begin{CodeOutput}
Joint mean-covariance model based on HPC ['jmcmMod']
Formula: weight | id | I(day/14 + 1) ~ 1 | 1
Poly: c(8, 2, 2)
Data: cattleA

logLik: -746.9001
BIC: 51.4939

Mean Parameters:
[1]   1.781e+02   1.373e+02  -1.528e+02   8.500e+01  -2.535e+01   4.359e+00
[7]  -4.330e-01   2.307e-02  -5.101e-04
Variance Parameters:
[1]   4.0263   0.3148  -0.0113
Angle Parameters:
[1]   0.729414   0.092111  -0.004424
  \end{CodeOutput}
\end{CodeChunk}

A slightly better result with $\hat{l}_{max}$ = -746.9001 and BIC = 51.4939 is
produced compared to reported $\hat{l}_{max}$ = -755.00 and BIC =
52.03. Similarly, model fitting can be checked by plot the two regressograms
using:
\begin{CodeChunk}
  \begin{CodeInput}
R> regressogram(fit3, time = 1:11)
  \end{CodeInput}
\end{CodeChunk}

We need to note that there is no general form for calculating angles. The
corresponding angles $\phi_{ijk}$ of the empirical correlation matrix is
calculated iteratively using expression
\begin{equation}
  \theta_{ijk} = \arccos(b_{ijk}/\prod_{l=1}^{k-1}\sin(\arccos(\theta_{ijl}))),
  \quad 1 \leq k < j \leq m_i,
\end{equation}
where $\prod_{1}^{0}$ is taken as 1. By examining the log variance versus time
(left) and angle versus time lag (right) in Figure~\ref{fig:hpcreg}, it is clear
that curvature patterns on two sample regressograms is well captured by the two
fitted model, indicating a good model fitting.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.80\textwidth]{hpcreg}
  \caption{\label{fig:hpcreg}Group A analysis of Kenward's cattle data. Sample
    regressograms and HPC model fits based on the triple ploy(8, 2, 2) for log
    variances (left) and angles (right).}
\end{figure}

The comparisons between MCD, ACD and HPC based joint mean-covariance models on
cattle data are made in Table~\ref{table:cattletable} with different choices of
triple and execution time (in seconds) are measured for each model fitting. We
find that HPC-based model is more desirable in most cases with a larger value in
log likelihood and smaller BIC when compared to MCD and ACD based models at the
cost of a much longer execution time. From Table~\ref{table:cattletable}, we
also find that MCD and ACD will produce quite close results in term of value of
likelihood and BIC while MCD based model is most time efficient among three
approaches. Our tests were conducted under Windows 10 (64-bit version) on
ThinkPad T410 equipped with an Intel(R) Core(TM) i5 M 480@2.67GHz with 4.00GB of
RAM.

\begin{center}
  \begin{table}[htbp]
    {\scriptsize
      \begin{tabular}{lrrrrrrrrrrrr}\\\hline
        (p,d,q) & No. of
        &\multicolumn{3}{c}{MCD}
        &\multicolumn{3}{c}{ACD}
        &\multicolumn{3}{c}{HPC}
        \\\cline{3-5}\cline{6-8}\cline{9-11}
        & parms.
        & $l_{max}$ & BIC & Time
        & $l_{max}$ & BIC & Time
        & $l_{max}$ & BIC & Time\\\hline
        (8,3,4) & 18
        & -771.0007 & 53.4408 & 0.53
        & -747.6994 & 51.8873 & 3.54
        & -745.2783 & 51.7259 & 6.61 \\
        (8,2,2) & 15
        & -789.6174 & 54.3418 & 0.57
        & -750.8567 & 51.7577 & 2.78
        & -746.9001 & 51.4939 & 6.28\\
        (10,10,10) & 33
        & -739.1477 & 53.0178 & 4.77
        & -740.7159 & 53.1224 & 88.68
        & -739.6031 & 53.0482 & 128.10\\
        (6,1,1) & 11
        & -823.8421 & 56.1699 & 0.47
        & -763.5859 & 52.1528 & 1.33
        & -759.5982 & 51.8870 & 4.09	\\
        (3,3,3) & 12
        & -825.3397 & 56.3831 & 1.05
        & -800.8213 & 54.7486 & 6.03
        & -798.1533 & 54.5707 & 16.69\\
        (4,4,3) & 14
        & -791.1545 & 54.3309 & 0.80
        & -760.6863 & 52.2996 & 3.21
        & -760.2976 & 52.2737 & 10.25\\
        (7,2,2) & 14
        & -791.7968 & 54.3737 & 0.47
        & -755.7579 & 51.9711 & 2.14
        & -751.8171 & 51.7084 & 6.17\\
        (8,7,4) & 22
        & -769.5302 & 53.7962 & 1.89
        & -745.1182 & 52.1688 & 4.97
        & -743.1843 & 52.0398 & 16.19\\
        (9,1,3) & 16
        & -794.7426 & 54.7968 & 0.38
        & -750.0146 & 51.8149 & 2.94
        & -746.7736 & 51.5989 & 9.39\\
        (9,4,3) & 19
        & -783.2143 & 54.3684 & 0.61
        & -746.3733 & 51.9123 & 5.41
        & -744.9879 & 51.8200 & 9.98\\
        (9,8,5) & 25
        & -754.3422 & 53.1238 & 2.09
        & -743.2145 & 52.3820 & 28.67
        & -741.6877 & 52.2802 & 37.91\\
        \hline
    \end{tabular}}
    \footnotesize
    \caption{\label{table:cattletable}Kenward's cattle data. Comparison of MCD,
      ACD and HPC with different triples.}
    \vspace{-2mm}
  \end{table}
\end{center}

\subsection{Analysis of an unbalanced longitudinal dataset}
\label{subsec:cd4unbal}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
In this section, we apply the proposed joint mean-covariance modelling approach
to an unbalanced CD4+ cell dataset analysed by \cite{ye2006modelling} and
\cite{zhang2015joint}. The dataset comprises a total of 2376 CD4+ cell counts of
369 HIV-infected men covering a period of approximately eight and half year. The
number of measurement $m_i$ for each individual varies from 1 to 12 and the
times are not equally spaced. The CD4+ cell data are highly unbalanced and
included in the package.

\begin{CodeChunk}
  \begin{CodeInput}
R> data("aids", package = "jmcm")
R> head(aids)
  \end{CodeInput}
  \begin{CodeOutput}
       time cd4  age packs drugs sex cesd    id
1 -0.741958 548 6.57     0     0   5    8 10002
2 -0.246407 893 6.57     0     1   5    2 10002
3  0.243669 657 6.57     0     1   5   -1 10002
4 -2.729637 464 6.95     0     1   5    4 10005
5 -2.250513 845 6.95     0     1   5   -4 10005
6 -0.221766 752 6.95     0     1   5   -5 10005
  \end{CodeOutput}
\end{CodeChunk}

We present in Figure~\ref{fig:aidsdata} the scatter plot of CD4+ cell counts
against time, with the first six individuals profiles superimposed:

\begin{CodeChunk}
  \begin{CodeInput}
R> library("lattice")
R> xyplot(sqrt(cd4) ~ time, data = aids,
     panel = function(x, y, ...) {
       panel.xyplot(x, y, ...)
       panel.lines(x[aids$id==10002], y[aids$id==10002], col = 2, lwd = 2)
       panel.lines(x[aids$id==10005], y[aids$id==10005], col = 3, lwd = 2)
       panel.lines(x[aids$id==10029], y[aids$id==10029], col = 4, lwd = 2)
       panel.lines(x[aids$id==10039], y[aids$id==10039], col = 5, lwd = 2)
       panel.lines(x[aids$id==10048], y[aids$id==10048], col = 6, lwd = 2)
       panel.lines(x[aids$id==10052], y[aids$id==10052], col = 7, lwd = 2)
     },
     xlab = "Time", ylab = "CD4 cell numbers", col = 1)
  \end{CodeInput}
\end{CodeChunk}

and observe that the data is highly unbalanced with unclear profile patterns for
each individual.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{aidsdata}
  \caption{\label{fig:aidsdata}Scatter plot of CD4+ cell counts against time,
    with the first six individuals profiles superimposed.}
\end{figure}

As in \cite{zhang2015joint}, square roots of the CD4 cell counts are used to
make the response variable closer to the Normal distribution. The optimal
triplet poly(8, 1, 3) of MCD method reported in \cite{zhang2015joint} is fitted
using the following command:

\begin{CodeChunk}
  \begin{CodeInput}
R> fit4 <- jmcm(I(sqrt(cd4)) | id | time ~ 1 | 1, data = aids,
+    triple = c(8, 1, 3), cov.method = 'mcd')	
R> fit4
  \end{CodeInput}
  \begin{CodeOutput}
Joint mean-covariance model based on MCD ['jmcmMod']
Formula: I(sqrt(cd4)) | id | time ~ 1 | 1
Poly: c(8, 1, 3)
Data: aids
	
logLik: -4979.193
BIC: 27.2278

Mean Parameters:
[1]  29.217447  -4.100596  -1.279396   1.073685   0.195578  -0.166439
[7]  -0.001842   0.009407  -0.001020
Innovation Variance Parameters:
[1]   3.2646  -0.0886
Autoregressive Parameters:
[1]   0.67990  -0.57684   0.17741  -0.01815
  \end{CodeOutput}
\end{CodeChunk}

Here the CD4 data is again re-analysed with MCD based joint mean-covariance
model using time as the main covariates and values of $\hat{l}_{max}$ =
-4979.193 and smaller BIC = 27.2278 are obtained. Note that
\code{jmcm} function do allow adding other covariates in the mean model and
innovation variance model. For instance, the linear formula part of \code{jmcm}
function in this example can be replaced by \code{I(sqrt(cd4)) | id | time ~ age
  | age + packs}, which in turn generates the new vectors of covariates for
the mean and innovation variance with the following form

\begin{align*}
  x_{ij} & = (1, t_{ij}, t_{ij}^2, \cdots, t_{ij}^p, age)^\top, \\
  z_{ij} & = (1, t_{ij}, t_{ij}^2, \cdots, t_{ij}^d, age, packs)^\top.
\end{align*}

The joint mean-covariance model based on ACD and HPC approaches can also be
fitted with other covariate in a similar way, and currently the fitted models
can be compared with other model fittings using the value of log likelihood and
BIC.

Since CD4+ cell data are unbalanced, sample covariance matrix cannot be obtained
and using instruction \code{regressogram()} with the model fitting result will
simply lead to an error message. Instead we produced fitted curves and its 95\%
confidence interval based on bootstrap using the following function:

\begin{CodeChunk}
  \begin{CodeInput}
R> bootcurve(fit4, nboot = 1000)
  \end{CodeInput}
\end{CodeChunk}

where number of bootstrap replications can be specified by option \code{nboot}
and there are 1000 bootstrap samples in this example. Figure~\ref{fig:mcdboot}
shows the fitted curve of the mean, log innovation variance, autoregressive
coefficient and their corresponding 95\% confidence intervals. From
Figure~\ref{fig:mcdboot}, we also observed the monotone-decreasing relationship
of fitted log innovation variance with the time, and a curvature pattern of
fitted autoregressive coefficient with the time lag.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{mcdboot}
  \caption{\label{fig:mcdboot} CD4 cell data. MCD model fits based on the triple
    poly(8,1,3). Fitted curves of the mean against time (top), the log
    innovation variance against time (left) and the autoregressive coefficient
    against lag (right): - - - - - -, 95\% confidence intervals.}
\end{figure}

The same triple poly(8,1,3) is used in joint mean-covariance model fitting based
on ACD for aids data, and we clearly see that ACD method produces a larger
likelihood $\hat{l}_{max}$ = -4927.492 and a smaller BIC-value 26.9476.

\begin{CodeChunk}
  \begin{CodeInput}
R> fit5 <- jmcm(I(sqrt(cd4)) | id | time ~ 1 | 1, data = aids,
+    triple = c(8, 1, 3), cov.method = 'acd')
R> fit5
  \end{CodeInput}
  \begin{CodeOutput}
Joint mean-covariance model based on ACD ['jmcmMod']
Formula: I(sqrt(cd4)) | id | time ~ 1 | 1
Poly: c(8, 1, 3)
Data: aids

logLik: -4927.492
BIC: 26.9476

Mean Parameters:
[1]  29.0395978  -4.0577291  -1.0767686   0.9747427   0.1479554  -0.1416514
[7]  -0.0002373   0.0076762  -0.0008416
Innovation Variance Parameters:
[1]   3.2441  -0.1163
Moving Average Parameters:
[1]   0.580633  -0.151159   0.056772  -0.006433
  \end{CodeOutput}
\end{CodeChunk}

Fitted curves for ACD can be produced by the same command:
\begin{CodeChunk}
  \begin{CodeInput}
R> bootcurve(fit5, nboot = 1000)
  \end{CodeInput}
\end{CodeChunk}

From Figure~\ref{fig:acdboot}, again we observed the monotone-decreasing
relationship of fitted log innovation variance with the time, and a curvature
pattern of fitted moving average coefficient with the time lag.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{acdboot}
  \caption{\label{fig:acdboot} CD4 cell data. ACD model fits based on the triple
    poly(8,1,3). Fitted curves of the mean against time (top), the log
    innovation variance against time (left) and the moving average coefficient
    against lag (right): - - - - - -, 95\% confidence intervals.}
\end{figure}	

When the optimal triplet poly(8, 1, 1) of HPC approach reported in
\cite{zhang2015joint} is fitted, the optimal BIC-value turns out to be 26.7268,
and $\hat{l}_{max}$ = -4892.68, producing the best model among the three
proposed covariance and correlation structure modelling methods.

\begin{CodeChunk}
  \begin{CodeInput}
R> fit6 <- jmcm(I(sqrt(cd4)) | id | time ~ 1 | 1, data = aids,
+    triple = c(8, 1, 1), cov.method = 'hpc')
R> fit6
  \end{CodeInput}
  \begin{CodeOutput}
Joint mean-covariance model based on HPC ['jmcmMod']
Formula: I(sqrt(cd4)) | id | time ~ 1 | 1
Poly: c(8, 1, 1)
Data: aids

logLik: -4892.68
BIC: 26.7268

Mean Parameters:
[1]  29.0352214  -4.1553878  -0.9452119   0.9969254   0.1066325  -0.1394301
[7]   0.0026802   0.0072015  -0.0008294
Variance Parameters:
[1]  3.64089  0.03252
Angle Parameters:
[1]  1.06980  0.05357
  \end{CodeOutput}
\end{CodeChunk}

Similarly, model fitting can be checked by plot the fitted curves using
\begin{CodeChunk}
  \begin{CodeInput}
R> bootcurve(fit6, nboot = 1000)
  \end{CodeInput}
\end{CodeChunk}

and from Figure~\ref{fig:hpcboot}, we observe the monotone increasing
relationship of fitted log variance with the time, and fitted angles with the
time lag.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{hpcboot}
  \caption{\label{fig:hpcboot} CD4 cell data. HPC model fits based on the triple
    poly(8,1,1). Fitted curves of the mean against time (top), the log variance
    against time (left) and the angle against lag (right): - - - - - -, 95\%
    confidence intervals.}
\end{figure}	

We also compared the MCD, ACD and HPC based joint mean-covariance models on the
CD4+ cell data in Table~\ref{table:cd4table}. We find that HPC-based model
proves better model fitting in most cases with a larger value in log likelihood
and smaller BIC when compared to MCD and ACD based models at the cost of a much
longer execution time, ACD based model slightly outperforms compared to MCD, and
MCD based model again provides the most time-efficient model fitting.

\begin{center}
  \begin{table}[htbp]
    {\scriptsize
      \begin{tabular}{lrrrrrrrrrrrr}\\\hline
        (p,d,q) & No. of
        &\multicolumn{3}{c}{MCD}
        &\multicolumn{3}{c}{ACD}
        &\multicolumn{3}{c}{HPC}
        \\\cline{3-5}\cline{6-8}\cline{9-11}
        & parms.
        & $l_{max}$ & BIC & Time
        & $l_{max}$ & BIC & Time
        & $l_{max}$ & BIC & Time\\\hline
        (8,1,1) & 13
        & -5008.753 & 27.3560 & 2.11
        & -4928.924 & 26.9233 & 37.28
        & -4892.680 & 26.7268 & 134.64 \\
        (8,1,3) & 15
        & -4979.193 & 27.2278 & 2.80
        & -4927.492 & 26.9476 & 39.72
        & -4890.396 & 26.7465 & 112.79\\
        (6,1,1) & 11
        & -5018.470 & 27.3766 & 1.88
        & -4937.227 & 26.9362 & 25.06
        & -4902.175 & 26.7463 & 79.32\\
        (3,3,3) & 12
        & -5006.176 & 27.3260 & 3.22
        & -4951.234 & 27.0282 & 40.57
        & -4919.522 & 26.8563 & 120.88\\
        (4,4,3) & 14
        & -4995.509 & 27.3002 & 3.36
        & -4934.265 & 26.9682 & 56.34
        & -4902.100 & 26.7939 & 177.25\\
        (8,3,3) & 17
        & -4974.683 & 27.2354 & 3.31
        & -4919.700 & 26.9374 & 58.68
        & -4886.337 & 26.7565 & 155.85\\
        (8,7,4) & 22
        & -4971.712 & 27.2994 & 7.16
        & -4914.223 & 26.9878 & 270.25
        & -4881.736 & 26.8117 & 763.83\\
        (9,1,3) & 16
        & -4974.104 & 27.2162 & 3.00
        & -4918.684 & 26.9158 & 63.18
        & -4881.266 & 26.7130 & 120.09\\
        (9,4,3) & 19
        & -4970.209 & 27.2432 & 5.06
        & -4909.363 & 26.9134 & 66.25
        & -4875.877 & 26.7319 & 212.51\\
        (9,8,5) & 25
        & -4962.655 & 27.2983 & 7.70
        & -4901.841 & 26.9687 & 221.10
        & -4871.577 & 26.8047 & 662.64\\
        \hline
    \end{tabular}}
    \footnotesize
    \caption{\label{table:cd4table}CD4 cell data. Comparison of MCD, ACD and HPC
      with different triples.}
    \vspace{-2mm}
  \end{table}
\end{center}

\section[conclusions]{Conclusion}
\label{sec:conclusions}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

In this article, we have illustrated the capabilities of package \pkg{jmcm} for
the joint mean-covariance modelling of both balanced and unbalanced longitudinal
data using three popular covariance and correlation structure modelling
approaches. In particular, we provide: functions for estimation of MCD, ACD and
HPC based joint mean-covariance models, devices for displaying regressograms and
fitted model curves. By using these models, the estimated covariance and
correlation are guaranteed to be positive (semi)definite and the estimation of
high-dimensional covariance and correlation matrix is reduced to solving a
series of regression problems. The likelihood-based estimation procedure permits
extensions such as regularization-based model selection, so that the package 
can be compared with other likelihood-based \proglang{R} packages.

However, the package is currently limited to handle longitudinal data with a
multivariate Gaussian distribution. It is worthwhile to develop methods further
that are robust with non-Normally distributed data by introducing the Cholesky-based
covariance structure modelling methods to GEE model and/or Gaussian copula model. 
We plan to update the package \pkg{jmcm} on a regular basis with new statistical
procedures available for joint mean-covariance modelling approach.


\section*{Acknowledgements}
We would like to thank the Editors and two anonymous reviewers for their constructive 
comments and helpful suggestions.

%\bibliographystyle{jss}
\bibliography{jss2542}

\end{document}
